{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Submitted by AVIK CHAKRABORTY\n",
    "## This task follows on the problem of data extraction from the web using web scrapping and then performing Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBJECTIVES:\n",
    "#### 1. Data Extration from the website provided in the Input.xlsx file.\n",
    "#### 2. Cleaning the extracted data by removing the STOPWORDS.\n",
    "#### 3. Perform: POSITIVE SCORE, NEGATIVE SCORE, POLARITY SCORE, SUBJECTIVITY SCORE, AVG SENTENCE LENGTH, PERCENTAGE OF COMPLEX WORDS, FOG INDEX, AVG NUMBER OF WORDS PER SENTENCE, COMPLEX WORD COUNT, WORD COUNT, SYLLABLE PER WORD, PERSONAL PRONOUNS, AVG WORD LENGTH\n",
    "#### 4. Save the file as the file structure in Output Data Structure.xlsx file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = p.read_excel('Input.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the duplicate or empty data from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.URL.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.URL_ID.empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXTRACTION via WEB SCRAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_class = ['tdb-title-text']\n",
    "text_class = ['tdb-block-inner td-fix-index', 'td-post-content tagdiv-type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Server side web messages displayed when the algorithm scrapes the data.\n",
    "##### Website no 36 and 49 cannot be scrapped as the server is NOT FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status_URL(url):\n",
    "    status = requests.get(url)\n",
    "\n",
    "    if status.status_code == 200:\n",
    "        return 'OK'\n",
    "    if status.status_code == 400:\n",
    "        return 'BAD REQUEST'\n",
    "    if status.status_code == 401:\n",
    "        return 'UNAUTHORIZED'\n",
    "    if status.status_code == 403:\n",
    "        return 'Forbidden'\n",
    "    if status.status_code == 404:\n",
    "        return 'NOT FOUND' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the HTML h1 tag's class for scrapping the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_HTMLelement_class(soup):\n",
    "    title_class.append(soup.find('h1').get('class')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the length of the article text for the sites if it shows 404 error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_length_ARTICLE_TEXT(article_text):\n",
    "    high = pos = 0\n",
    "    \n",
    "    for i in range(len(article_text)):\n",
    "        if high < len(article_text[i]):\n",
    "            high = len(article_text[i])\n",
    "            pos = i\n",
    "\n",
    "    return article_text[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the extracted data in a file in a sperate forlder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_data(article_title, article_text, file_name):\n",
    "    with open(file = str('extracted files/' + file_name + '.txt'), mode = 'wb') as file:\n",
    "        file.write(article_title)\n",
    "        file.write(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting the data from the web using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracting_data(soup, file_name):\n",
    "    finding_HTMLelement_class(soup)\n",
    "\n",
    "    for i in title_class:\n",
    "        if soup.find('h1', i):\n",
    "            # encoding the text in utf-8 form as get_text() cannot encode all the letters\n",
    "            article_title = soup.find('h1', i).get_text().encode('utf-8')\n",
    "            break\n",
    "\n",
    "    article_text = []\n",
    "    for i in text_class:\n",
    "        if soup.find('div', i):\n",
    "            # article_text = soup.find_all('div', i).get_text().encode('utf-8')\n",
    "            for tags in soup.find_all('div', i):\n",
    "                article_text.append(tags.get_text().encode('utf-8'))\n",
    "\n",
    "            article_text = check_length_ARTICLE_TEXT(article_text)\n",
    "            break\n",
    "\n",
    "    saving_data(article_title, article_text, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accessing the sites in the Input.xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALl the files have been saved !\n"
     ]
    }
   ],
   "source": [
    "for ind in range(97,98,1):\n",
    "    url = data.iloc[ind, 1]\n",
    "\n",
    "    check_status = check_status_URL(url)\n",
    "\n",
    "    if check_status == 'OK':\n",
    "        html = urlopen(url).read()\n",
    "        soup = BeautifulSoup(html, features = 'html.parser').find('article')\n",
    "\n",
    "        # killing all html scripts and style elements.\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.extract() # ripping the scripts out\n",
    "\n",
    "        extracting_data(soup, data.iloc[ind, 0])\n",
    "\n",
    "        if check_status == 'BAD REQUEST':\n",
    "            print(f'URL at index {ind} is a {check_status} !')\n",
    "        \n",
    "        if check_status == 'UNAUTHORIZED':\n",
    "            print(f'URL at index {ind} is {check_status} !')\n",
    "\n",
    "        if check_status == 'FORBIDDEN':\n",
    "            print(f'URL at index {ind} is {check_status} !')\n",
    "\n",
    "        if check_status == 'NOT FOUND':\n",
    "            print(f'URL at index {ind} is {check_status} !')\n",
    "\n",
    "print('All the files have been saved !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing the lines in the file into seperate lines and storing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving changed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_reformed_files(full_text, file):\n",
    "    with open(file = 'changed files/' + file, mode = 'w', encoding = 'utf-8') as fp:\n",
    "        fp.write(full_text)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seperating the file lines by the terminators ['.', '?', '!'] and storing the seperate lines as a full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformed_files(file_text, file):\n",
    "    full_text = ''\n",
    "    for sentence in file_text:\n",
    "        words = word_tokenize(sentence)\n",
    "        \n",
    "        line = ''\n",
    "        for word in words:\n",
    "            if word in ['?', '.', '!']:\n",
    "                line += '.\\n'\n",
    "            else:\n",
    "                line = line + ' ' + word\n",
    "\n",
    "        full_text += line\n",
    "\n",
    "    saving_reformed_files(full_text, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Opening the file and reading each line of the file.\n",
    "##### The file is read line by line. But the problem with the readlines() is that it accepts the paragraph as a single line as seen here. Therefore we need to change the .txt in a way that all the lines are read as seperate lines for future work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('extracted files'):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(file = 'extracted_files/' + file, mode = 'r', encoding = 'utf-8') as fp:\n",
    "            file_text = fp.readlines()\n",
    "            fp.close()\n",
    "\n",
    "        reformed_files(file_text, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding and storing all the STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "\n",
    "for file in os.listdir('StopWords'):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(file = 'StopWords/' + file, mode = 'r') as fp:\n",
    "            for word in fp.read().split():\n",
    "                stop_words.append(word)\n",
    "            fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the cleaned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_file(full_text, file):\n",
    "    with open('cleaned files/' + file, mode = 'w', encoding = 'utf-8') as fp:\n",
    "        fp.write(full_text)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('changed files'):\n",
    "    if file.endswith('.txt'):\n",
    "        with open(file = 'changed files/' + file, mode = 'r', encoding = 'utf-8') as fp:\n",
    "            file_text = fp.readlines()\n",
    "            fp.close()\n",
    "\n",
    "        full_text = ''\n",
    "        for lines in file_text:\n",
    "            words = word_tokenize(lines)\n",
    "            \n",
    "            words_copy = words.copy()\n",
    "            for word in words_copy:\n",
    "                if word in stop_words:\n",
    "                    words.remove(word)\n",
    "\n",
    "            full_text += ' '.join(words) + '\\n'\n",
    "\n",
    "            save_cleaned_file(full_text, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_score_perFile = []\n",
    "negative_score_perFile = []\n",
    "polarity_score_perFile = []\n",
    "subjectivity_score_perFile = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the positive, negative, polarity, subjectivity score to the Output.xlsx File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_PNPoSub_score(data):\n",
    "    # since file no 36 does not exits\n",
    "    positive_score_perFile.insert(35, 0)\n",
    "    negative_score_perFile.insert(35, 0)\n",
    "    subjectivity_score_perFile.insert(35, 0)\n",
    "    polarity_score_perFile.insert(35, 0)\n",
    "    \n",
    "    # since file no 49 does not exits\n",
    "    positive_score_perFile.insert(48, 0)\n",
    "    negative_score_perFile.insert(48, 0)\n",
    "    subjectivity_score_perFile.insert(48, 0)\n",
    "    polarity_score_perFile.insert(48, 0)\n",
    "\n",
    "    data['POSITIVE SCORE'] = positive_score_perFile\n",
    "    data['NEGATIVE SCORE'] = negative_score_perFile\n",
    "    data['POLARITY SCORE'] = polarity_score_perFile\n",
    "    data['SUBJECTIVITY SCORE'] = subjectivity_score_perFile\n",
    "\n",
    "    data.to_excel('Output.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the positive and negative words of each file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_PN_words(positive_words, negative_words):\n",
    "    positive_words = '\\n'.join(positive_words)\n",
    "    negative_words = '\\n'.join(negative_words)\n",
    "\n",
    "    with open('MasterDictionary/' + 'positive-words.txt', mode = 'a', encoding = 'utf-8') as fp:\n",
    "        fp.write('\\n' + positive_words)\n",
    "        fp.close()\n",
    "\n",
    "    with open('MasterDictionary/' + 'negative-words.txt', mode = 'a', encoding = 'utf-8') as fp:\n",
    "        fp.write('\\n' + negative_words)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the Polarity Score and Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_polarityANDsubjectivity_score(positive_score, negative_score, file):\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "    with open('cleaned files/' + file, mode = 'r', encoding = 'utf-8') as fp:\n",
    "        file_text = fp.read().split()\n",
    "        fp.close()\n",
    "\n",
    "    count = 0\n",
    "    for word in file_text:\n",
    "        if ord(word[0]) in range(65, 91, 1) or ord(word[0]) in range(97, 123, 1):\n",
    "            count += 1\n",
    "\n",
    "    subjectivity_score = (positive_score + negative_score) / (count + 0.000001)\n",
    "\n",
    "    polarity_score_perFile.append(polarity_score)\n",
    "    subjectivity_score_perFile.append(subjectivity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the positive and negative words from the cleaned files and storing them\n",
    "##### Also, finding the positive and negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_PN_wordsANDscore(file_text, positive_words_file, negative_words_file, file):\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "\n",
    "    for lines in file_text:\n",
    "        words = word_tokenize(lines)\n",
    "\n",
    "        # the score is calculated after each word is found\n",
    "        for word in words:\n",
    "            if word in positive_words_file:\n",
    "                positive_score += 1\n",
    "                positive_words.append(word)\n",
    "        \n",
    "        for word in words:\n",
    "            if word in negative_words_file:\n",
    "                negative_score -= 1\n",
    "                negative_words.append(word)\n",
    "\n",
    "\n",
    "    negative_score = negative_score * (-1)\n",
    "\n",
    "    positive_score_perFile.append(positive_score)\n",
    "    negative_score_perFile.append(negative_score)\n",
    "    \n",
    "    # we do not need to repeat the same word and save it therefore set() is used\n",
    "    positive_words = list(set(positive_words))\n",
    "    negative_words = list(set(negative_words))\n",
    "\n",
    "    finding_polarityANDsubjectivity_score(positive_score, negative_score, file)\n",
    "    save_PN_words(positive_words, negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fetching and storing the positive and negative words from the Master Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = p.read_excel('Output.xlsx')\n",
    "\n",
    "for file in os.listdir('cleaned files'):\n",
    "    if file.endswith('.txt'):\n",
    "        with open('MasterDictionary/' + 'positive-words.txt', mode = 'r', encoding = 'utf-8') as fp:\n",
    "            positive_words_file = fp.read().split()\n",
    "            fp.close()\n",
    "\n",
    "        with open('MasterDictionary/' + 'negative-words.txt', mode = 'r') as fp:\n",
    "            negative_words_file = fp.read().split()\n",
    "            fp.close()\n",
    "\n",
    "        file_text = ''\n",
    "        with open('cleaned files/' + file, mode = 'r', encoding = 'utf-8') as fp:\n",
    "            file_text = fp.readlines()\n",
    "            fp.close()\n",
    "\n",
    "        finding_PN_wordsANDscore(file_text, positive_words_file, negative_words_file, file)\n",
    "\n",
    "save_PNPoSub_score(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GLobal Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_perFile = []\n",
    "avg_sent_length_perFile = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length(count, file_sentences):\n",
    "    avg_sent_length = count // len(file_sentences)\n",
    "\n",
    "    avg_sent_length_perFile.append(avg_sent_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the Percentage of Complex Words per file\n",
    "##### Removing symbols and counting the words per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_file_words(file_words, file_sentences):\n",
    "    count = 0\n",
    "\n",
    "    for word in file_words:\n",
    "        if ord(word[0]) in range(65, 91, 1) or ord(word[0]) in range(97, 123, 1):\n",
    "            count += 1\n",
    "\n",
    "    word_count_perFile.append(count)\n",
    "    \n",
    "    average_sentence_length(count, file_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the CMU dictionary\n",
    "d = cmudict.dict()\n",
    "\n",
    "# opening each file\n",
    "for file in os.listdir('cleaned files'):\n",
    "    with open(file = 'cleaned files/' + file, mode = 'r', encoding = 'utf-8') as fp:\n",
    "        file_words = fp.read().split()\n",
    "        fp.close()\n",
    "        \n",
    "    with open(file = 'cleaned files/' + file, mode = 'r', encoding = 'utf-8') as fp:\n",
    "        file_sentences = fp.readlines()\n",
    "        fp.close()\n",
    "\n",
    "    count_file_words(file_words, file_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberof_complex_words_perFile = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    if word.lower() in d:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    else:\n",
    "        # Fallback: consider one syllable if the word is not in the dictionary\n",
    "        return 1\n",
    "\n",
    "# Define what makes a word complex (e.g., more than 3 syllables or more than 7 characters)\n",
    "def is_complex_word(word):\n",
    "    return len(word) > 7 or count_syllables(word) > 3\n",
    "\n",
    "# Function to read a file and check each word\n",
    "def find_complex_words(filename):\n",
    "    with open('cleaned files/' + filename, mode = 'r', encoding = 'utf-8') as fp:\n",
    "        text = fp.read()\n",
    "\n",
    "    # Use regex to find words in the text\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    # Find and print complex words\n",
    "    complex_words_perFile = [word for word in words if is_complex_word(word)]\n",
    "    return complex_words_perFile\n",
    "\n",
    "# Opening the files\n",
    "for file in os.listdir('cleaned files'):\n",
    "    complex_words_perFile = find_complex_words(file)\n",
    "    numberof_complex_words_perFile.append(len(complex_words_perFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Percentage of complex words per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_complex_word_perFile = []\n",
    "\n",
    "for pos in range(0, 98, 1):\n",
    "    percentage_complex_word_perFile.append(numberof_complex_words_perFile[pos] / word_count_perFile[pos] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fog_index_perFile = []\n",
    "\n",
    "for pos in range(0, 98, 1):\n",
    "    fog_index_perFile.append(avg_sent_length_perFile[pos] + percentage_complex_word_perFile[pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the Average Sentence Length, Percentage of Complex Words, Fog Index, Word Count, Complex Word Count in Output.xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since file 36 does not exists\n",
    "avg_sent_length_perFile.insert(35, 0)\n",
    "percentage_complex_word_perFile.insert(35, 0)\n",
    "fog_index_perFile.insert(35, 0)\n",
    "word_count_perFile.insert(35, 0)\n",
    "numberof_complex_words_perFile.insert(35, 0)\n",
    "\n",
    "# Since file 49 does not exists\n",
    "avg_sent_length_perFile.insert(48, 0)\n",
    "percentage_complex_word_perFile.insert(48, 0)\n",
    "fog_index_perFile.insert(48, 0)\n",
    "word_count_perFile.insert(48, 0)\n",
    "numberof_complex_words_perFile.insert(48, 0)\n",
    "\n",
    "data = p.read_excel('Output.xlsx')\n",
    "\n",
    "data['AVG SENTENCE LENGTH'] = avg_sent_length_perFile\n",
    "data['PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_word_perFile\n",
    "data['FOG INDEX'] = fog_index_perFile\n",
    "data['WORD COUNT'] = word_count_perFile\n",
    "data['COMPLEX WORD COUNT'] = numberof_complex_words_perFile\n",
    "\n",
    "data.to_excel('Output.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the Syllable count per word in each file\n",
    "##### GLobal Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_count_perFile = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Counting the syllables in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting_syllables_perFile(words):\n",
    "    syllable_count_perWord = []\n",
    "\n",
    "    for word in words:\n",
    "        count = 0\n",
    "        for letter in word:\n",
    "            if letter in ['a', 'e', 'i', 'o', 'u']:\n",
    "                count += 1\n",
    "\n",
    "        syllable_count_perWord.append(count)\n",
    "\n",
    "    return syllable_count_perWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing symbols and filtering words that is ending with es and ed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(file_text):\n",
    "    words = []\n",
    "\n",
    "    for word in file_text:\n",
    "        if ord(word[0]) in range(65, 90, 1) or ord(word[0]) in range(97, 122, 1):\n",
    "            # handling the ed and es exception.\n",
    "            # reversing the string as negative indexing is being used.\n",
    "            if word[-1:-3:-1] not in ['se', 'de']:\n",
    "                words.append(word)\n",
    "\n",
    "    syllable_count_perWord = counting_syllables_perFile(words)\n",
    "\n",
    "    return words, syllable_count_perWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating the syllable count per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('cleaned files'):\n",
    "    with open(file = 'cleaned files/' + file, mode = 'r', encoding = 'utf-8') as fp:\n",
    "        file_text = fp.read().split()\n",
    "        fp.close()\n",
    "\n",
    "    words, syllable_count_perWord = remove_symbols(file_text)\n",
    "\n",
    "    '''Since it is mentioned that we need to count syllables in each word,\n",
    "    Also in the output file the column is syllable per word.\n",
    "    Therefore the average syllable count per word in each file is calculated and stored.'''\n",
    "    syllable_count_perFile.append(sum(syllable_count_perWord) // len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the syllable in Output.xlsx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since file 38 an 49 does not exists\n",
    "syllable_count_perFile.insert(35, 0)\n",
    "syllable_count_perFile.insert(48, 0)\n",
    "\n",
    "data = p.read_excel('Output.xlsx')\n",
    "\n",
    "data['SYLLABLE PER WORD'] = syllable_count_perFile\n",
    "\n",
    "data.to_excel('Output.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating number of personal pronouns and average word length per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_pronouns = r'\\b(I|me|my|mine|we|us|our|ours|you|your|yours|he|him|his|she|her|hers|it|its|they|them|their|theirs)\\b'\n",
    "\n",
    "personal_pronouns_perFile = []\n",
    "avg_word_length_perFile = []\n",
    "\n",
    "for file in os.listdir('cleaned files'):\n",
    "    with open(file = 'cleaned files/' + file, mode = 'r', encoding = 'utf-8') as fp:\n",
    "        file_text1 = fp.read()\n",
    "        fp.close()\n",
    "        \n",
    "    with open(file = 'cleaned files/' + file, mode = 'r', encoding = 'utf-8') as fp:\n",
    "        file_text2 = fp.read().split()\n",
    "        fp.close()\n",
    "\n",
    "    words = []\n",
    "    for word in file_text2:\n",
    "        if ord(word[0]) in range(65, 90, 1) or ord(word[0]) in range(97, 122, 1):\n",
    "            words.append(word)\n",
    "\n",
    "    each_word_length = []\n",
    "    for word in words:\n",
    "        each_word_length.append(len(word))\n",
    "\n",
    "    avg_word_length_perFile.append(sum(each_word_length) // len(words) if len(words) != 0 else 0)\n",
    "\n",
    "    words_pp = re.findall(personal_pronouns, file_text1)\n",
    "    personal_pronouns_perFile.append(len(words_pp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the personal pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file 36 and 49 does not exists\n",
    "personal_pronouns_perFile.insert(35, 0)\n",
    "avg_word_length_perFile.insert(35, 0)\n",
    "personal_pronouns_perFile.insert(48, 0)\n",
    "avg_word_length_perFile.insert(48, 0)\n",
    "\n",
    "data = p.read_excel('Output.xlsx')\n",
    "data['PERSONAL PRONOUNS'] = personal_pronouns_perFile\n",
    "data['AVG WORD LENGTH'] = avg_word_length_perFile\n",
    "\n",
    "data.to_excel('Output.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
